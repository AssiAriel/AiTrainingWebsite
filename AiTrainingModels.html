<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Training - Models</title>
    <link rel="icon" href="favicon.png" type="image/png">
    <link rel="stylesheet" href="style.css"> 
</head>

<!----------------------------------------------------------------------------------------------->

<body>
    <p class="TitleHead">AI Training - Models</p>  
	
   <!--- Models Training -->
	<p class="TitleList1Blue">Models Training</p>
    <ol>

		<li>
	    <p class="TitleList2Blue">Historical Modles review</p>
        <ol>
	    	<li>
    		<p><a href="https://www.youtube.com/watch?v=OFS90-FX6pg" target="_blank">The 35 Year History of ChatGPT (Art of the Problem)</a></p>
	        <div class="video-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/OFS90-FX6pg" frameborder="0" allowfullscreen></iframe>
            </div>
	    	<p>The above "The 35 Year History of ChatGPT" video lists the historical published papers.</p>
	    	</li>
        </ol>
		</li>

	    <li>
		<p class="TitleList2Blue">Transformer (and RNR)</p>
    	<p>This Transformer (the published paper) relates to a Translation application and not to a Generative model - 
	    <br>it has the basics of a Generative model but the architecture is different.</p>
        <ol>
            <li>
			<p><a href="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf" target="_blank">Attention Is All You Need</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf"></iframe>
            </div>
    	    <p>The published Paper by Google for Transformer.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=bCz4OMemCcA" target="_blank">Attention is all you need (Transformer) - Model explanation (including math), Inference and Training (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/bCz4OMemCcA" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>Transformer basic explanation - specifically the 'Attention is all you need' paper.</p>
		    </li>
        </ol>
        </li>
		
	    <li>
		<p class="TitleList2Blue">GPT-1</p>
		<ol>
			<li>
    		<p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Improving Language Understanding by Generative Pre-Training</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-1.</p>
            </li>
        </ol>
        </li>

		<li>
		<p class="TitleList2Blue">GPT-2</p>
		<ol>
			<li>
	    	<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">Language Models are Unsupervised Multitask Learners</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"></iframe>
            </div>
        	<p>The published Paper by OpenAI for GPT-2.</p>
            </li>
        </ol>
        </li>

	    <li>
		<p class="TitleList2Blue">GPT-3</p>
        <ol>
            <li>
			<p><a href="https://arxiv.org/pdf/2005.14165" target="_blank">Language Models are Few-Shot Learners</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://arxiv.org/pdf/2005.14165"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-3.
			<br>Note that the paper includes lots of tests details that are not so important - and are covered in short in the video.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=SY5PvZrJhLE" target="_blank">GPT-3: Language Models are Few-Shot Learners (Paper Explained) (Yannic Kilcher)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/SY5PvZrJhLE" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>GPT-3 paper basic explanation.</p>
		    </li>
        </ol>
	    </li>

	    <li>
		<p class="TitleList2Blue">GPT-4</p>
        <ol>
            <li>
			<p><a href="https://cdn.openai.com/papers/gpt-4.pdf" target="_blank">GPT-4 Technical Report</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/papers/gpt-4.pdf"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-4.
			<br>Note that the paper includes lots of tests results which are not so important.
			<br>Also note that the second part of the paper (starting on page 41 "GPT-4 System Card") is about safty issues.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=2AdkSYWB6LY" target="_blank">GPT 4: Full Breakdown (14 Details You May Have Missed) (AI Explained)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/2AdkSYWB6LY" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>GPT-4 paper basic explanation.</p>
		    </li>
        </ol>
	    </li>

	    <li>
		<p class="TitleList2Blue">DeepSeek</p>
        <ol>
            <li>
			<p><a href="https://arxiv.org/pdf/2501.12948" target="_blank">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://arxiv.org/pdf/2501.12948"></iframe>
            </div>
    	    <p>The published Paper by DeepSeek for DeepSeek-R1.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc" target="_blank">Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/XMnxKGVnEUc" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>DeepSeek-R1 paper explanation.</p>
		    </li>
        </ol>
	    </li>

	    <li>
		<p class="TitleList2Blue">LLaMA 1</p>
        <ol>
            <li>
			<p><a href="https://arxiv.org/pdf/2302.13971" target="_blank">LLaMA: Open and Efficient Foundation Language Models</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://arxiv.org/pdf/2302.13971"></iframe>
            </div>
    	    <p>The published Paper by Meta AI for LLaMA 1.
			<br>Note that the paper includes lots of tests details that are not so important.</p>
			</li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=E5OnoYF2oAk" target="_blank">LLaMA: Open and Efficient Foundation Language Models (Paper Explained) (Yannic Kilcher)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/E5OnoYF2oAk" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>LLaMA 1 paper general explanation.</p>
		    </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo" target="_blank">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/Mn_9W1nCFLo" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>LLaMA 1 & 2 Technolgy basic explanation.</p>
			</li>
        </ol>
	    </li>




	    <li>
		<p class="TitleList2Blue">LLaMA 2</p>
        <ol>

			

           <li>
paper 2 3?
			
			</li>


			<li>
video - for llama 2 or 3?

			</li>


			<li>
video - code llama, here or in code?


			</li>



        </ol>
	    </li>

		




		
		<li>
		<p class="TitleList2Blue">BERT</p>
		<ol>
			<li>
		    <p><a href="https://www.youtube.com/watch?v=90mGPxR2GgY" target="_blank">BERT explained: Training, Inference, BERT vs GPT/LLamA, Fine tuning, [CLS] token (Umar Jamil)</a></p>
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/90mGPxR2GgY" frameborder="0" allowfullscreen></iframe>
		    <p>BERT basic differences explanation.</p>
		    </li>
		</ol>
		</li>

  		<li>
		<p class="TitleList2Blue">Mistral</p>
		<ol>
			<li>
     		<p><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE" target="_blank">Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer (Umar Jamil)</a></p>
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/UiX8K-xBUpE" frameborder="0" allowfullscreen></iframe>
		    <p>Mistral basic differences explanation.</p>
		    </li>
		</ol>
		</li>

    </ol>
 	

</body>
</html>
