<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Training - Advanced</title>
    <link rel="icon" href="favicon.png" type="image/png">
    <link rel="stylesheet" href="style.css"> 
</head>

<!----------------------------------------------------------------------------------------------->

<body>
    <p class="TitleHead">AI Training - Advanced</p>  
	
   <!--- Advanced Training - Models -->
	<p class="TitleList1Blue">Advanced Training - Models</p>
    <ol>

		<li>
	    <p class="TitleList2Blue">Historical Modles review</p>
        <ol>
	    	<li>
    		<p><a href="https://www.youtube.com/watch?v=OFS90-FX6pg" target="_blank">The 35 Year History of ChatGPT (Art of the Problem)</a></p>
	        <div class="video-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/OFS90-FX6pg" frameborder="0" allowfullscreen></iframe>
            </div>
	    	<p>The above "The 35 Year History of ChatGPT" video lists the historical published papers.</p>
	    	</li>
        </ol>
		</li>

	    <li>
		<p class="TitleList2Blue">Transformer (and RNR)</p>
    	<p>This Transformer (the published paper) relates to a Translation application and not to a Generative model - 
	    <br>it has the basics of a Generative model but the architecture is different.</p>
        <ol>
            <li>
			<p><a href="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf" target="_blank">Attention Is All You Need</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf"></iframe>
            </div>
    	    <p>The published Paper by Google for Transformer.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=bCz4OMemCcA" target="_blank">Attention is all you need (Transformer) - Model explanation (including math), Inference and Training (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/bCz4OMemCcA" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>Transformer basic explanation - specifically the 'Attention is all you need' paper.</p>
		    </li>
        </ol>
        </li>
		
	    <li>
		<p class="TitleList2Blue">GPT-1</p>
		<ol>
			<li>
    		<p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Improving Language Understanding by Generative Pre-Training</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-1.</p>
            </li>
        </ol>
        </li>

		<li>
		<p class="TitleList2Blue">GPT-2</p>
		<ol>
			<li>
	    	<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">Language Models are Unsupervised Multitask Learners</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"></iframe>
            </div>
        	<p>The published Paper by OpenAI for GPT-2.</p>
            </li>
        </ol>
        </li>

	    <li>
		<p class="TitleList2Blue">GPT-3</p>
        <ol>
            <li>
			<p><a href="https://arxiv.org/pdf/2005.14165" target="_blank">Language Models are Few-Shot Learners</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://arxiv.org/pdf/2005.14165"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-3.
			<br>Note that the paper includes lots of tests details that are not so important - and are covered in short in the video.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=SY5PvZrJhLE" target="_blank">GPT-3: Language Models are Few-Shot Learners (Paper Explained) (Yannic Kilcher)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/SY5PvZrJhLE" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>GPT-3 paper basic explanation.</p>
		    </li>
        </ol>
	    </li>

	    <li>
		<p class="TitleList2Blue">GPT-4</p>
        <ol>
            <li>
			<p><a href="https://cdn.openai.com/papers/gpt-4.pdf" target="_blank">GPT-4 Technical Report</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://cdn.openai.com/papers/gpt-4.pdf"></iframe>
            </div>
    	    <p>The published Paper by OpenAI for GPT-4.
			<br>Note that the paper includes lots of tests results which are not so important.
			<br>Also note that the second part of the paper (starting on page 41 "GPT-4 System Card") is about safty issues.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=2AdkSYWB6LY" target="_blank">GPT 4: Full Breakdown (14 Details You May Have Missed) (AI Explained)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/2AdkSYWB6LY" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>GPT-4 paper basic explanation.</p>
		    </li>
        </ol>
	    </li>

	    <li>
		<p class="TitleList2Blue">DeepSeek</p>
        <ol>
            <li>
			<p><a href="https://arxiv.org/pdf/2501.12948" target="_blank">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://arxiv.org/pdf/2501.12948"></iframe>
            </div>
    	    <p>The published Paper by DeepSeek for DeepSeek-R1.</p>
            </li>

			<li>
			<p><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc" target="_blank">Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/XMnxKGVnEUc" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>DeepSeek-R1 paper explanation.</p>
		    </li>
        </ol>
	    </li>

<br>LLama

	
		<li>
		<p class="TitleList2Blue">BERT</p>
		<ol>
			<li>
		    <p><a href="https://www.youtube.com/watch?v=90mGPxR2GgY" target="_blank">BERT explained: Training, Inference, BERT vs GPT/LLamA, Fine tuning, [CLS] token (Umar Jamil)</a></p>
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/90mGPxR2GgY" frameborder="0" allowfullscreen></iframe>
		    <p>BERT - basic differences explanation.</p>
		    </li>
		</ol>
		</li>

  		<li>
		<p class="TitleList2Blue">Mistral</p>
		<ol>
			<li>
     		<p><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE" target="_blank">Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer (Umar Jamil)</a></p>
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/UiX8K-xBUpE" frameborder="0" allowfullscreen></iframe>
		    <p>Mistral - basic differences explanation.</p>
		    </li>
		</ol>
		</li>

    </ol>
  	
<!----------------------------------------------------------------------------------------------->

    <!--- Advanced Training - Code -->
    <p class="TitleList1Blue">Advanced Training - Code</p>
    <div class="video-container">
    <ol>
		<li>
		<p class="TitleList2Blue">BatchNorm and optimization</p>
		<ol>
			<li>
			<a href="https://www.youtube.com/watch?v=P6sfmUTpUmc" target="_blank">Building makemore Part 3: Activations & Gradients, BatchNorm (Andrej Karpathy)</a>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/P6sfmUTpUmc" frameborder="0" allowfullscreen></iframe>
            </div>
 	        <p>Part 3: BatchNorm and optimization - Character generation example code.
 	        <br>Note that we skip videos part 4 and part 5 - they are very optional.</p>
		    </li>
		</ol>
		
		<li>
		<p class="TitleList2Blue">Tokenizer</p>
        <ol>
			<li>
		    <a href="https://www.youtube.com/watch?v=zduSFxRajkE" target="_blank">Let's build the GPT Tokenizer (Andrej Karpathy)</a>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/zduSFxRajkE" frameborder="0" allowfullscreen></iframe>
            </div>
			</li>
		</ol>
		</li>

	    <li>
		<p class="TitleList2Blue">GPT-2 (some GPT-3) and GPU optimization</p>
        <ol>
            <li>
			<p><a href="https://www.youtube.com/watch?v=l8pRSuU81PU" target="_blank">Let's reproduce GPT-2 (124M) (Andrej Karpathy)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/l8pRSuU81PU" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>GPT-2 (some GPT-3) code and running with GPU optimization.</p>
		    </li>

	        <li>
            <a href="https://github.com/karpathy/build-nanogpt" target="_blank">GitHub: github.com/karpathy/build-nanogpt</a>
        	<p>GitHub of the above GPT-2 project.</p>
            </li>
        </ol>
        </li>

    </ol>
    </div>

</body>
</html>
