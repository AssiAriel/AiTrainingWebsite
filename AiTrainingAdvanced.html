<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Training - Advanced</title>
    <link rel="icon" href="favicon.png" type="image/png">
    <link rel="stylesheet" href="style.css"> 
</head>

<!----------------------------------------------------------------------------------------------->

<body>
    <p class="TitleHead">AI Training - Advanced</p>  
	
   <!--- Advanced Training - Papers -->
	<p class="TitleList1Blue">Advanced Training - Papers</p>
    <ol>
	    <li>
		<p class="TitleList2Blue">Transformer (and RNR)</p>
    	<p>This Transformer (the published paper) relates to a Translation application and not to a Generative model - 
	    <br>it has the basics of a Generative model but the architecture is different.</p>
        <ol>
            <li>
			<p><a href="https://www.youtube.com/watch?v=bCz4OMemCcA" target="_blank">Attention is all you need (Transformer) - Model explanation (including math), Inference and Training (Umar Jamil)</a></p>
            <div class="preview-container">
	        <iframe width="300" height="170" src="https://www.youtube.com/embed/bCz4OMemCcA" frameborder="0" allowfullscreen></iframe>
            </div>
 	     	<p>Transformer basic explanation - specifically the 'Attention is all you need' paper.</p>
		    </li>
			
            <li>
			<p><a href="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf" target="_blank">Attention Is All You Need</a></p>
            <div class="iframe-wrapper">
            <iframe src="https://nanonets.com/webflow-bundles/feb23update/RAG_for_PDFs/build_v6/attention-is-all-you-need.pdf"></iframe>
            </div>
    	    <p>The published Paper by Google for Transformer.</p>
            </li>
        </ol>
        </li>
		
	    <li>
		<p class="TitleList2Blue">GPT-1</p>
		<p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Improving Language Understanding by Generative Pre-Training</a></p>
        <div class="iframe-wrapper">
        <iframe src="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"></iframe>
        </div>
    	<p>The published Paper by OPenAI for GPT-1.</p>
        </li>

		
    </ol>

<!----------------------------------------------------------------------------------------------->

    <!--- Advanced Training - Videos -->
    <p class="TitleList1Blue">Advanced Training - Videos</p>
    <div class="video-container">
    <ol>
		<li>
		<a href="https://www.youtube.com/watch?v=OFS90-FX6pg" target="_blank">The 35 Year History of ChatGPT (Art of the Problem)</a>
	    <iframe width="300" height="170" src="https://www.youtube.com/embed/OFS90-FX6pg" frameborder="0" allowfullscreen></iframe>
		<p>The above "The 35 Year History of ChatGPT" video lists the historical published papers.</p>
		</li>
		
		<li>
		<a href="https://www.youtube.com/watch?v=90mGPxR2GgY" target="_blank">BERT explained: Training, Inference, BERT vs GPT/LLamA, Fine tuning, [CLS] token (Umar Jamil)</a>
	    <iframe width="300" height="170" src="https://www.youtube.com/embed/90mGPxR2GgY" frameborder="0" allowfullscreen></iframe>
		<p>BERT - basic differences explanation.</p>
		</li>

  		<li>
		<a href="https://www.youtube.com/watch?v=UiX8K-xBUpE" target="_blank">Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer (Umar Jamil)</a>
	    <iframe width="300" height="170" src="https://www.youtube.com/embed/UiX8K-xBUpE" frameborder="0" allowfullscreen></iframe>
		<p>Mistral - basic differences explanation.</p>
		</li>
		
    </ol>
    </div>
  	
<!----------------------------------------------------------------------------------------------->

    <!--- Advanced Training - Code -->
    <p class="TitleList1Blue">Advanced Training - Code</p>
    <div class="video-container">
    <ol>
		<li>
		<a href="https://www.youtube.com/watch?v=P6sfmUTpUmc" target="_blank">Building makemore Part 3: Activations & Gradients, BatchNorm (Andrej Karpathy)</a>
        <div class="preview-container">
	    <iframe width="300" height="170" src="https://www.youtube.com/embed/P6sfmUTpUmc" frameborder="0" allowfullscreen></iframe>
        </div>
 	    <p>Part 3: BatchNorm and optimization - Character generation example code.
 	    <br>Note that we skip videos part 4 and part 5 - they are very optional.</p>
		</li>

		<li>
		<a href="https://www.youtube.com/watch?v=zduSFxRajkE" target="_blank">Let's build the GPT Tokenizer (Andrej Karpathy)</a>
        <div class="preview-container">
	    <iframe width="300" height="170" src="https://www.youtube.com/embed/zduSFxRajkE" frameborder="0" allowfullscreen></iframe>
        </div>
		</li>

 ........ 
	Let's reproduce GPT-2 (124M)


    </ol>
    </div>

</body>
</html>

<!----------------------------------------------------------------------------------------------->
